{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMulhJiC2qWWWFj1eyRIKC/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yasmine-90/am02/blob/main/s%C3%ADntese_dos_seis_exemplos_apresentados_fine_tuning_e_destila%C3%A7%C3%A3o_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fining Tuning e Destilação**\n",
        "\n",
        " São duas técnicas usadas no treinamento e adaptação de modelos de aprendizado de máquina. Elas têm objetivos diferentes, mas muitas vezes são usadas em conjunto para melhorar o desempenho, eficiência ou especialização de um modelo."
      ],
      "metadata": {
        "id": "rf7izIJ2CBJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tuning (Ajuste fino)**\n",
        "\n",
        "Fine tuning é o processo de pegar um modelo pré-treinado e continuar o treinamento dele com um novo conjunto de dados, geralmente menor e mais específico.\n",
        "\n",
        "O modelo é treinado com dados reais anotados."
      ],
      "metadata": {
        "id": "eqsT-xC_DAEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Destilação (Knowledge Distillation)**\n",
        "\n",
        "Destilação é o processo de treinar um modelo menor (o \"aluno\") para imitar o comportamento de um modelo maior e mais complexo (o \"professor\").\n",
        "\n",
        "O modelo aluno aprende com as saídas (previsões) do modelo professor."
      ],
      "metadata": {
        "id": "ChTSv8siDm2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exemplos Fine Tuning**\n"
      ],
      "metadata": {
        "id": "awrujlOHI40s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Fine-Tuning Llama 2 em Dados Personalizados:**\n",
        "\n",
        " O objetivo é usar o modelo Llama-2 7B LLM para realizar Resposta a Perguntas (Question Answering) em dados privados.\n",
        "\n",
        " A memória da GPU necessária para executar o tutorial, 14GB GPU RAM é suficiente para esse tutorial. Dependendo da GPU que você está usando, uma GPU pode ser suficiente (por exemplo, a NVIDIA RTX 4090 tem 24 GB de RAM).\n",
        "\n",
        "No caso o código de exemplo é definido automaticamente a GPU comoo T4..\n",
        "\n",
        "A GPU padrão disponível no Colab gratuitamente é uma NVIDIA T4.\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "Passos do código com Llama 2:\n",
        "\n",
        "1.   Configurar o ambiente: Verificar e configurar a GPU, instalar bibliotecas necessárias.\n",
        "\n",
        "2.   Obter acesso ao modelo Llama-2: O texto explica como fazer isso no Hugging Face.\n",
        "\n",
        "3.   Carregar e configurar o modelo Llama-2: O código importa as classes necessárias para isso.\n",
        "4.   Preparar os dados privados: O texto menciona a capacidade de usar texto livre, indicando que o notebook terá etapas para carregar e processar esses dados.\n",
        "5.  Ajustar fino o modelo Llama-2 nos dados privados: O código importa bibliotecas (peft, bitsandbytes) que são usadas para otimizar o processo de ajuste fino, tornando-o mais eficiente em termos de recursos.\n",
        "6.  Usar o modelo ajustado para Resposta a Perguntas: A seção final do notebook provavelmente demonstrará como o modelo ajustado pode responder a perguntas com base nos dados privados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ewQawQxDJ8Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Fine-Tuning Mistral-7b com QLoRA:**\n",
        "\n",
        "O código também define automaticamente a GPU como T4.\n",
        "\n",
        "Mistral-7B é um modelo de linguagem de código aberto com 7 bilhões de parâmetros, lançado pela startup francesa Mistral AI.  \n",
        "\n",
        "QLoRA (Quantized Low-Rank Adapter) é uma técnica que torna o fine-tuning de LLMs muito mais leve e barato.\n",
        "\n",
        "-------------------------------------------------------\n",
        "\n",
        "Passos do código com QLoRA:\n",
        "\n",
        "1.  Introdução/Configuração: Instala bibliotecas e configura o ambiente para ajustar o Mistral-7b com SFT e QLoRA.\n",
        "2.  Preparação: Carrega e divide o conjunto de dados. Configura QLoRA e LoRA. Carrega o tokenizador e o modelo base quantizado.\n",
        "3.  Treinamento: Treina o modelo usando SFT com os dados e configurações definidas.\n",
        "4.  Teste: Testa o modelo ajustado com um prompt de exemplo para gerar texto.\n",
        "5.  Fusão: Funde o adaptador LoRA treinado com o modelo base para criar um modelo único.\n",
        "6.  Publicação (Opcional): Faz upload do modelo ajustado e tokenizador para o Hugging Face Hub.\n",
        "7.  Recursos Adicionais: Sugere próximos passos e ferramentas para ajuste de LLMs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s432CQhYWqK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Fine-Tuning Google Gemini com LoRA:**\n",
        "\n",
        "Vantagens de usar LoRA com Gemini:\n",
        "\n",
        "Economia de recursos - Só treina poucos milhões de parâmetros, não bilhões\n",
        "\n",
        "Rápido -\tFine-tuning pode levar minutos ou poucas horas\n",
        "\n",
        "Customizável -\tPermite adaptar o Gemini a dados ou tarefas específicas\n",
        "\n",
        "Sem degradar o original -\tO modelo base permanece intacto\n",
        "\n",
        "LoRA (Low-Rank Adaptation) é uma técnica que permite treinar apenas pequenas partes adaptativas de um modelo grande, sem precisar modificar todos os seus bilhões de parâmetros.\n",
        "\n",
        "A LoRA injeta camadas extras (matrizes de baixo-rank) nas atenções do modelo e treina só essas partes.\n",
        "\n",
        "Isso é muito mais leve, rápido e barato que treinar o modelo inteiro.\n",
        "\n",
        "\n",
        "---------------------------------------------------------------\n",
        "\n",
        "Passos do código com Lora:\n",
        "\n",
        "1. Configuração:\n",
        "Instala a biblioteca Python para interagir com a API Gemini.\n",
        "Importa as bibliotecas de programação necessárias.\n",
        "Configura a API Gemini usando sua chave de API.\n",
        "\n",
        "2. Inspeção de Modelos:\n",
        "Permite verificar os modelos afinados que você já possui.\n",
        "Mostra como encontrar um modelo base adequado para usar na afinação.\n",
        "\n",
        "3. Criação e Monitoramento da Afinação do Modelo:\n",
        "Explica como criar um modelo afinado, passando seu próprio conjunto de dados de treinamento.\n",
        "Informa que o modelo afinado é adicionado à lista, mas inicialmente está no status \"creating\" (criando) enquanto a afinação acontece.\n",
        "Descreve como verificar o progresso da afinação.\n",
        "Explica como aguardar a conclusão do processo de afinação.\n",
        "Menciona a opção de cancelar a tarefa de afinação.\n",
        "\n",
        "4. Análise da Afinação:\n",
        "Explica que após a conclusão da afinação, você pode visualizar a \"curva de perda\", que mostra o quanto as previsões do modelo se desviam das saídas corretas durante o treinamento.\n",
        "\n",
        "5. Avaliação do Modelo Afinada:\n",
        "Descreve como usar o modelo afinado recém-criado para gerar texto (a próxima sequência numérica).\n",
        "Mostra exemplos de como o modelo se comporta com diferentes tipos de entrada.\n",
        "Observa que o modelo parece ter aprendido a tarefa apesar dos poucos exemplos, mas que \"próximo\" é um conceito simples, e a documentação fornece mais orientações para melhorar o desempenho em tarefas mais complexas.\n",
        "\n",
        "6. Gerenciamento do Modelo:\n",
        "Explica como atualizar a descrição do seu modelo afinado.\n",
        "Descreve como excluir modelos afinados que você não precisa mais para limpar sua lista.\n",
        "Menciona a conveniência de excluir trabalhos de afinação cancelados, pois seu desempenho pode ser imprevisível.\n",
        "Mostra que o modelo não existe mais após a exclusão."
      ],
      "metadata": {
        "id": "1TZCSXx1bJiD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6hTKSE5vceKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exemplos de Destilação**\n"
      ],
      "metadata": {
        "id": "8ZYqHsUgcgkU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Destilação de Conhecimento com Keras para Visão Computacional:**\n",
        "\n",
        "A destilação de conhecimento (knowledge distillation) é uma técnica de aprendizado de máquina onde:\n",
        "\n",
        "Um modelo grande e poderoso (chamado modelo professor) ensina um modelo menor e mais leve (chamado modelo aluno).\n",
        "\n",
        "O modelo aluno aprende não só com os rótulos reais das imagens, mas também com as previsões do modelo professor (as \"soft labels\").\n",
        "\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "Passo do código:\n",
        "\n",
        "1.  Introdução à Destilação de Conhecimento: A seção inicial fornece uma breve explicação sobre o que é a Destilação de Conhecimento e seu objetivo (compressão de modelo). Menciona o conceito de \"temperatura\" para suavizar logits e a referência ao artigo original.\n",
        "2.  Configuração: Esta seção importa as bibliotecas Python e Keras/TensorFlow necessárias para o restante do notebook.\n",
        "3.  Construção da Classe Distiller(): Define uma classe Python personalizada que encapsula a lógica para a destilação de conhecimento. Esta classe estende keras.Model e implementa métodos para compilar o processo de treinamento e calcular a perda, incorporando as perdas do estudante e de destilação.\n",
        "4.  Criação dos Modelos Estudante e Professor: Define as arquiteturas das redes neurais convolucionais para o modelo professor (maior) e o modelo estudante (menor). Um modelo estudante é clonado para ser treinado do zero posteriormente para fins de comparação.\n",
        "5.  Preparação do Conjunto de Dados: Carrega o conjunto de dados MNIST e realiza o pré-processamento necessário, como normalização e redimensionamento.\n",
        "\n",
        "6.  Treinamento do Professor: O modelo professor é compilado e treinado no conjunto de dados de treinamento. Em seguida, é avaliado no conjunto de dados de teste para estabelecer um desempenho de referência.\n",
        "7.  Destilação do Professor para o Estudante: Uma instância da classe Distiller é criada com os modelos estudante e professor. O destilador é compilado com as funções de perda e hiperparâmetros apropriados. O processo de destilação é executado, treinando o modelo estudante com a orientação do modelo professor. O estudante destilado é então avaliado no conjunto de teste.\n",
        "8.  Treinamento do Estudante do Zero para Comparação: O modelo estudante clonado (não destilado) é compilado e treinado no conjunto de dados de treinamento independentemente. Este modelo também é avaliado no conjunto de dados de teste para comparar seu desempenho com o estudante destilado.\n",
        "9.  Observações Finais: O notebook termina com um comentário que resume os resultados esperados, indicando que o estudante destilado deve ter um desempenho melhor do que o estudante treinado do zero e possivelmente melhor do que o professor em termos de precisão.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O8PR2Y7ZcnFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tutorial de Destilação de Conhecimento com PyTorch:**\n",
        "--------------------------------------------------------------\n",
        "\n",
        "Passos do código com PyTorch:\n",
        "\n",
        "1.  Setup: Configurar o ambiente e carregar o dataset CIFAR-10.\n",
        "2.  Modelos: Definir duas redes neurais: uma mais profunda (professor) e uma mais leve (aluno).\n",
        "3.  Baseline: Treinar e avaliar o modelo professor e o modelo aluno apenas com a perda de entropia cruzada para estabelecer uma baseline.\n",
        "4.  Destilação de Logits: Implementar e aplicar a destilação de conhecimento clássica, adicionando uma perda baseada na semelhança entre os logits de saída do professor e do aluno.\n",
        "5.  Minimização de Perda de Cosseno: Explorar outra abordagem onde a perda é calculada entre as representações ocultas dos modelos, usando a perda de cosseno. Novos modelos com saídas modificadas são criados para isso.\n",
        "6.  Regressor Intermediário: Implementar um método mais avançado usando um regressor intermediário no aluno para fazer com que os mapas de características do aluno se pareçam com os do professor, utilizando a perda MSE. Novos modelos modificados são usados novamente.\n",
        "7.  Comparação: Comparar as acurácias de todos os modelos alunos treinados com diferentes métodos de destilação em relação à baseline e ao professor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HS7LSAc4ytbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Destilação de Conhecimento com TensorFlow:**\n",
        "----------------------------------------------------\n",
        "\n",
        "Passos do código com TensorFlow:\n",
        "\n",
        "1.  Preparar: Introduz o conceito de Knowledge Distillation, instala e importa bibliotecas, carrega e pré-processa os dados.\n",
        "2.  Treinar Professor: Treina um modelo maior (professor) nos dados e salva seu melhor desempenho.\n",
        "3.  Preparar para Destilação: Demonstra o efeito da temperatura, cria um modelo professor para gerar saídas suavizadas e gera novos rótulos de treinamento combinando rótulos rígidos e saídas suaves do professor.\n",
        "4.  Treinar Estudante com KD: Define um modelo menor (estudante), define uma função de perda combinada (entropia cruzada + divergência KL) e treina o estudante usando os rótulos combinados e a função de perda de Knowledge Distillation. Salva e avalia o estudante treinado com KD.\n",
        "5.  Treinar Estudante Baseline: Treina o mesmo modelo estudante, mas de forma independente, apenas nos rótulos rígidos originais, como uma linha de base para comparação. Salva e avalia este estudante independente.\n",
        "6.  Finalizar: Lista as referências usadas.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cmH2nsUr1kN7"
      }
    }
  ]
}